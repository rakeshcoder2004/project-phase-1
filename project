 ##Webcam-based Eye Blink Analysis for Screen-induced Strain 


## Problem 

Screen-time has exponentially increased for a lot of us ever since we started working from home. While it may not always be possible to minimize this screen-time, we can take conscious steps to take care of our eyes. Spending too many hours staring at a screen can cause eye strain. We tend to blink less while staring at the blue light from a screen, and the movement of the screen makes your eyes work harder to focus. We typically do not position the screen at an ideal distance or angle, which can cause added strain. All these issues add up and can lead to lasting effects on vision: eye fatigue, dry and irritated eyes, loss of focus flexibility, retinal damage, etc.

One of the steps you can take, is to blink regularly. The average adult blinks between 10-20 times per minute. On an average we only blink 3-8 times per minute when reading, watching TV, listening to a podcast, working on a computer, or another activity that requires intense focus. That’s roughly 60 percent less than our normal rate of blinking!


## Solution

Eye Strain Detector to the rescue! It is a computer-vision based webapp that continuously monitors your blinking rate, in real time. If you don't blink enough, it reminds you to do so through desktop notifications. 

### Key Features 

- Instantly see your eye blinking rate
- Realtime eye strain alerts (notification & sound)
- The Computer Vision models used are robust and error rate is very low for different expressions, angles and even for people wearing spectacles[2]
- Click on the notifications to see more information!
- The app will work in the background and send reminders even if you don't open the browser
- If your eyes are closed for a prolonged period of time (5-6 secs), blinks are not detected


## How To Use

Note: Currently, the webapp has not been deployed but this can be cloned and used easily. Further, only supports MacOS as of now. 

To clone and run this application, you'll need [Git](https://git-scm.com), [Python](https://www.python.org/) and Anaconda 3 installed on your computer. From your command line:

```
"""
blink_counter.py

Counts blinks from a video file or webcam using dlib facial landmarks and EAR.
Saves blink timestamps to CSV and shows live count on the video window.

Usage examples:
    python blink_counter.py                # use webcam (default)
    python blink_counter.py --video path/to/my_blink01.mp4

Requirements:
    pip install opencv-python dlib imutils scipy
    (On Windows, install a compatible dlib wheel if pip install dlib fails.)

Put shape_predictor_68_face_landmarks.dat in the same folder or pass --model to point to it.
"""

import os
import csv
import argparse
import datetime

import cv2
import dlib
import imutils
from scipy.spatial import distance as dist
from imutils import face_utils


def calculate_EAR(eye):
    """Compute the eye aspect ratio for a single eye (6 landmarks)."""
    A = dist.euclidean(eye[1], eye[5])
    B = dist.euclidean(eye[2], eye[4])
    C = dist.euclidean(eye[0], eye[3])
    # Standard EAR formula
    ear = (A + B) / (2.0 * C)
    return ear


def main(args):
    # Parameters (tweak if needed)
    model_path = args.model
    video_path = args.video
    EAR_THRESH = args.threshold
    MIN_CONSEC_FRAMES = args.min_frames
    csv_out = args.csv

    if not os.path.isfile(model_path):
        raise FileNotFoundError(f"Facial landmark model not found at '{model_path}'.")

    # Initialize dlib's face detector and landmark predictor
    detector = dlib.get_frontal_face_detector()
    predictor = dlib.shape_predictor(model_path)

    (L_start, L_end) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
    (R_start, R_end) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]

    # Open video (0 for webcam)
    cam = cv2.VideoCapture(video_path)
    if not cam.isOpened():
        raise IOError(f"Could not open video source: {video_path}")

    frame_counter = 0  # frames below EAR threshold
    total_blinks = 0
    blink_events = []  # list of (timestamp_str, total_blinks)

    fps = cam.get(cv2.CAP_PROP_FPS) or 30.0

    while True:
        ret, frame = cam.read()
        if not ret:
            # End of file or camera disconnected
            break

        frame = imutils.resize(frame, width=640)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        faces = detector(gray, 0)
        for face in faces:
            shape = predictor(gray, face)
            shape = face_utils.shape_to_np(shape)

            leftEye = shape[L_start:L_end]
            rightEye = shape[R_start:R_end]

            leftEAR = calculate_EAR(leftEye)
            rightEAR = calculate_EAR(rightEye)
            ear = (leftEAR + rightEAR) / 2.0

            # visualization
            leftHull = cv2.convexHull(leftEye)
            rightHull = cv2.convexHull(rightEye)
            cv2.drawContours(frame, [leftHull], -1, (0, 255, 0), 1)
            cv2.drawContours(frame, [rightHull], -1, (0, 255, 0), 1)

            # blink logic
            if ear < EAR_THRESH:
                frame_counter += 1
            else:
                if frame_counter >= MIN_CONSEC_FRAMES:
                    total_blinks += 1
                    # compute timestamp for this blink (approx using frame index)
                    current_frame_idx = cam.get(cv2.CAP_PROP_POS_FRAMES)
                    seconds = current_frame_idx / fps
                    ts = str(datetime.timedelta(seconds=int(seconds)))
                    blink_events.append((ts, total_blinks))
                    # show immediate feedback on frame
                    cv2.putText(frame, "Blink Detected", (10, 30),
                                cv2.FONT_HERSHEY_DUPLEX, 0.8, (0, 200, 0), 2)
                frame_counter = 0

            # display EAR
            cv2.putText(frame, f"EAR: {ear:.3f}", (430, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

        # Display blink count
        cv2.putText(frame, f"Blinks: {total_blinks}", (430, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

        cv2.imshow("Blink Counter", frame)
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break

    cam.release()
    cv2.destroyAllWindows()

    # Save blink events to CSV if requested
    if csv_out:
        with open(csv_out, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["timestamp", "blink_count"])  # header
            for ts, cnt in blink_events:
                writer.writerow([ts, cnt])

        print(f"Saved {len(blink_events)} blink events to '{csv_out}'")

    print(f"Total blinks counted: {total_blinks}")


if _name_ == '_main_':
    ap = argparse.ArgumentParser()
    ap.add_argument("--video", type=str, default=0,
                    help="Path to video file or integer camera index. Use 0 for webcam.")
    ap.add_argument("--model", type=str, default="shape_predictor_68_face_landmarks.dat",
                    help="Path to dlib 68-face-landmark model (.dat)")
    ap.add_argument("--threshold", type=float, default=0.25,
                    help="EAR threshold to detect closed eye (typical 0.20-0.30)")
    ap.add_argument("--min-frames", type=int, default=2,
                    help="Minimum consecutive frames EAR must be below threshold to count as blink")
    ap.add_argument("--csv", type=str, default="blink_events.csv",
                    help="If set, saves blink timestamps to this CSV file (default: blink_events.csv)")
    args = ap.parse_args([])

    # convert video arg to int when appropriate (webcam)
    try:
        # if user passed 0 or '0', use camera index 0
        if str(args.video).isdigit():
            args.video = int(args.video)
    except Exception:
        pass

    main(args)
    ```


## Approach

The blink detector computes a metric called the eye aspect ratio (EAR), introduced by Soukupová and Čech in their 2016 paper, Real-Time Eye Blink Detection Using Facial Landmarks[1]. The eye aspect ratio makes for an elegant algorithm that involves a very simple calculation based on the ratio of distances between facial landmarks of the eyes. 

Each eye is represented by 6 (x, y)-coordinates, starting at the left-corner of the eye, and then working clockwise around the remainder of the region:

![blink_detection_6_landmarks](https://user-images.githubusercontent.com/37685052/91079233-6ccfdf00-e661-11ea-8804-25269701d328.jpg) 

Based on this image, we can see find a relation between the width and the height of these coordinates. We can then derive an equation that reflects this relation called the eye aspect ratio (EAR): 

![blink_detection_equation](https://user-images.githubusercontent.com/37685052/91079328-8a04ad80-e661-11ea-90b7-01d89fad71d2.png)

The eye aspect ratio is approximately constant while the eye is open, but will rapidly fall to zero when a blink is taking place. Using this simple equation, we can avoid image processing techniques and simply rely on the ratio of eye landmark distances to determine if a person is blinking. A frame threshold range is used to ensure that the person actually blinked and that their eyes are not closed for a long time.

![blink_detection_plot](https://user-images.githubusercontent.com/37685052/91079315-87a25380-e661-11ea-9f03-9c32bee8f9cc.jpg)

In this project, I have used existing Deep Learning models that detect faces and facial landmarks from images/video streams. These return the coordinates of the facial features like left eye, right eye, nose, etc. which have been used to calculate EAR. Blinking rate is monitored per minute.


## Demo 

You can view a demo of this project here : https://youtu.be/Tt2DR8FvYDk

## Scope for improvement & future plans

1. Currenly, EAR is the only quantitative metric used to determine if a person has blinked. However, due to noise in a video stream, subpar facial landmark detections, or fast changes in viewing angle, it could produce a false-positive detection, reporting that a blink had taken place when in reality the person had not blinked. To improve the blink detector, Soukupová and Čech recommend constructing a 13-dim feature vector of eye aspect ratios (N-th frame, N – 6 frames, and N + 6 frames), followed by feeding this feature vector into a Linear SVM for classification.

2. The UI is still being improved, and the web-app will be deployed soon.

3. Currently, this only works for MacOS due to some library limilations. It will be made cross platform soon.

4. Visualizations will be added so users can see insights about their blinking habits.


### References

1. [Research Paper: Real-Time Eye Blink Detection Using Facial Landmarks](http://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf)
2. [Facial and Landmark Recognition Models](http://dlib.net/)
3. [Creating web-application using Flask](https://towardsdatascience.com/designing-a-machine-learning-model-and-deploying-it-using-flask-on-heroku-9558ce6bde7b)
4. [HTML+CSS](https://templatemo.com/tag/video)
5. [Eye Health knowledge](https://visionsource.com/blog/are-you-blinking-enough/)
